{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d432f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>padded_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The staff was very kind and attentive to my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the staff was very kind and attentive to my needs</td>\n",
       "      <td>[101, 1996, 3095, 2001, 2200, 2785, 1998, 2012...</td>\n",
       "      <td>[101, 1996, 3095, 2001, 2200, 2785, 1998, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The waiting time was too long, and the staff w...</td>\n",
       "      <td>2</td>\n",
       "      <td>the waiting time was too long and the staff wa...</td>\n",
       "      <td>[101, 1996, 3403, 2051, 2001, 2205, 2146, 1998...</td>\n",
       "      <td>[101, 1996, 3403, 2051, 2001, 2205, 2146, 1998...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The doctor answered all my questions...but the...</td>\n",
       "      <td>1</td>\n",
       "      <td>the doctor answered all my questionsbut the fa...</td>\n",
       "      <td>[101, 1996, 3460, 4660, 2035, 2026, 3980, 8569...</td>\n",
       "      <td>[101, 1996, 3460, 4660, 2035, 2026, 3980, 8569...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The nurse was compassionate &amp; made me feel com...</td>\n",
       "      <td>0</td>\n",
       "      <td>the nurse was compassionate made me feel comfo...</td>\n",
       "      <td>[101, 1996, 6821, 2001, 29353, 2081, 2033, 251...</td>\n",
       "      <td>[101, 1996, 6821, 2001, 29353, 2081, 2033, 251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had to wait over an hour before being seen. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>i had to wait over an hour before being seen u...</td>\n",
       "      <td>[101, 1045, 2018, 2000, 3524, 2058, 2019, 3178...</td>\n",
       "      <td>[101, 1045, 2018, 2000, 3524, 2058, 2019, 3178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0    The staff was very kind and attentive to my ...      0   \n",
       "1  The waiting time was too long, and the staff w...      2   \n",
       "2  The doctor answered all my questions...but the...      1   \n",
       "3  The nurse was compassionate & made me feel com...      0   \n",
       "4  I had to wait over an hour before being seen. ...      2   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  the staff was very kind and attentive to my needs   \n",
       "1  the waiting time was too long and the staff wa...   \n",
       "2  the doctor answered all my questionsbut the fa...   \n",
       "3  the nurse was compassionate made me feel comfo...   \n",
       "4  i had to wait over an hour before being seen u...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [101, 1996, 3095, 2001, 2200, 2785, 1998, 2012...   \n",
       "1  [101, 1996, 3403, 2051, 2001, 2205, 2146, 1998...   \n",
       "2  [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569...   \n",
       "3  [101, 1996, 6821, 2001, 29353, 2081, 2033, 251...   \n",
       "4  [101, 1045, 2018, 2000, 3524, 2058, 2019, 3178...   \n",
       "\n",
       "                                    padded_tokenized  \n",
       "0  [101, 1996, 3095, 2001, 2200, 2785, 1998, 2012...  \n",
       "1  [101, 1996, 3403, 2051, 2001, 2205, 2146, 1998...  \n",
       "2  [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569...  \n",
       "3  [101, 1996, 6821, 2001, 29353, 2081, 2033, 251...  \n",
       "4  [101, 1045, 2018, 2000, 3524, 2058, 2019, 3178...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create a noisy dataset\n",
    "data_dict = {\n",
    "    \"text\": [\n",
    "        \"  The staff was very kind and attentive to my needs!!!  \",\n",
    "        \"The waiting time was too long, and the staff was rude. Visit us at http://hospitalreviews.com\",\n",
    "        \"The doctor answered all my questions...but the facility was outdated.   \",\n",
    "        \"The nurse was compassionate & made me feel comfortable!! :) \",\n",
    "        \"I had to wait over an hour before being seen.  Unacceptable service! #frustrated\",\n",
    "        \"The check-in process was smooth, but the doctor seemed rushed. Visit https://feedback.com\",\n",
    "        \"Everyone I interacted with was professional and helpful.  \"\n",
    "    ],\n",
    "    \"label\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
    "}\n",
    "\n",
    "# Convert dataset to a DataFrame\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "data[\"cleaned_text\"] = data[\"text\"].apply(clean_text)\n",
    "\n",
    "# Convert labels to integers\n",
    "label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
    "data[\"label\"] = data[\"label\"].map(label_map)\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "data['tokenized'] = data['cleaned_text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# Pad or truncate to fixed length (e.g., 128 tokens)\n",
    "data['padded_tokenized'] = data['tokenized'].apply(\n",
    "    lambda x: x + [tokenizer.pad_token_id] * (128 - len(x)) if len(x) < 128 else x[:128]\n",
    ")\n",
    "\n",
    "# Preview cleaned and labeled data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b0aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 4, Validation Size: 1, Test Size: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 70% training, 15% validation, 15% test\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training Size: {len(train_data)}, Validation Size: {len(val_data)}, Test Size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af91df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54896e9efd66479195001e97e5121583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effc78cffa2e4da5b6665d82814ae0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af476379298148bd83ca1669f00aa8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb8a896fb3f45e28d4ee0832770be59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be187c360d9c4c7a9dd25f425bf17cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1884a34409bb4f57a4489be4ff9a5f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'tokenized': [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569, 2102, 1996, 4322, 2001, 25963, 102], 'padded_tokenized': [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569, 2102, 1996, 4322, 2001, 25963, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '__index_level_0__': 2, 'input_ids': [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569, 2102, 1996, 4322, 2001, 25963, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"text\", \"cleaned_text\"])\n",
    "val_dataset = val_dataset.remove_columns([\"text\", \"cleaned_text\"])\n",
    "test_dataset = test_dataset.remove_columns([\"text\", \"cleaned_text\"])\n",
    "\n",
    "# Convert labels to int if they are not already\n",
    "train_dataset = train_dataset.map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "val_dataset = val_dataset.map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "test_dataset = test_dataset.map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "\n",
    "# Print a sample to confirm input_ids exist\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b928782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 11:00:02.064480: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-21 11:00:02.075525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755766802.088076 2433630 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755766802.092235 2433630 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755766802.102644 2433630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755766802.102657 2433630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755766802.102658 2433630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755766802.102659 2433630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-21 11:00:02.105747: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",  \n",
    "    logging_dir='./logs',  \n",
    "    save_strategy=\"epoch\",  \n",
    "    load_best_model_at_end=True \n",
    ")\n",
    "\n",
    "# **Explain 'evaluation_strategy':** \n",
    "# This determines when the model is evaluated. 'Epoch' evaluates the model after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a3c2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.061000</td>\n",
       "      <td>1.234752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.050500</td>\n",
       "      <td>1.274582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.955800</td>\n",
       "      <td>1.291182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.0224297642707825, metrics={'train_runtime': 3.9981, 'train_samples_per_second': 3.001, 'train_steps_per_second': 0.75, 'total_flos': 789340253184.0, 'train_loss': 1.0224297642707825, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]),\n",
    "    eval_dataset=val_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9b2d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5, F1 Score: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Generate predictions\n",
    "test_dataset = test_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = test_dataset[\"label\"]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")\n",
    "\n",
    "# **Explain metric importance**:\n",
    "# High F1 scores indicate balanced performance across all classes, crucial in tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5712e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
